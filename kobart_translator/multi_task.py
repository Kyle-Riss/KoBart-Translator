"""
Multi-Task KoBART Architecture
- Shared Encoder (1ê°œ)
- 4ê°œì˜ Task-specific Decoder Heads
"""

import copy
from typing import Dict, Optional

import torch
import torch.nn as nn
from transformers import BartConfig, BartForConditionalGeneration, PreTrainedTokenizerFast


class MultiTaskKoBART(nn.Module):
    """
    KoBART ê¸°ë°˜ ë©€í‹°íƒœìŠ¤í¬ ëª¨ë¸
    
    êµ¬ì¡°:
    - Shared Encoder (KoBART ì¸ì½”ë”)
    - 4ê°œì˜ Decoder Heads:
        1. Style Transfer
        2. Dialogue Summarization
        3. Role-conditioned Generation
        4. QA Answer Generation
    """
    
    def __init__(
        self,
        model_name: str = 'gogamza/kobart-base-v1',
        encoder_layers: Optional[int] = None,
        decoder_layers: Optional[int] = None,
        ffn_dim: Optional[int] = None,
        num_attention_heads: Optional[int] = None,
        gradient_checkpointing: bool = False,
    ):
        super(MultiTaskKoBART, self).__init__()
        
        print("Multi-Task KoBART ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
        
        config = BartConfig.from_pretrained(model_name)
        if encoder_layers is not None:
            config.encoder_layers = encoder_layers
        if decoder_layers is not None:
            config.decoder_layers = decoder_layers
        if num_attention_heads is not None:
            if config.d_model % num_attention_heads != 0:
                raise ValueError("d_model must be divisible by num_attention_heads.")
            config.encoder_attention_heads = num_attention_heads
            config.decoder_attention_heads = num_attention_heads

        base_model = BartForConditionalGeneration.from_pretrained(
            model_name,
            config=config,
        )

        if encoder_layers is not None and encoder_layers < len(base_model.model.encoder.layers):
            base_model.model.encoder.layers = nn.ModuleList(
                list(base_model.model.encoder.layers)[:encoder_layers]
            )
        if decoder_layers is not None and decoder_layers < len(base_model.model.decoder.layers):
            base_model.model.decoder.layers = nn.ModuleList(
                list(base_model.model.decoder.layers)[:decoder_layers]
            )

        if ffn_dim is not None:
            self._apply_ffn_reduction(base_model, ffn_dim)
            base_model.config.encoder_ffn_dim = ffn_dim
            base_model.config.decoder_ffn_dim = ffn_dim

        if gradient_checkpointing and hasattr(base_model, "gradient_checkpointing_enable"):
            base_model.gradient_checkpointing_enable()
        self.gradient_checkpointing = gradient_checkpointing
        
        # Shared Encoder (ê³µìœ  ì¸ì½”ë”)
        self.shared_encoder = base_model.model.encoder
        if self.gradient_checkpointing and hasattr(self.shared_encoder, "gradient_checkpointing"):
            self.shared_encoder.gradient_checkpointing = True
        print("[OK] Shared Encoder ë¡œë“œ ì™„ë£Œ")
        
        # ê¸°ë³¸ ë””ì½”ë” ì„¤ì • ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        decoder_config = base_model.model.decoder.config
        
        # íƒœìŠ¤í¬ ê·¸ë£¹ ì •ì˜: style/summary/roleì€ ê³µìœ  ë””ì½”ë”, QAëŠ” ë³„ë„ ë””ì½”ë”
        self.decoder_groups = {
            'shared_text': ['style_transfer', 'dialogue_summarization', 'role_generation'],
            'qa_generation': ['qa_generation'],
        }
        self.task_to_decoder = {
            task: group for group, tasks in self.decoder_groups.items() for task in tasks
        }
        
        self.decoders = nn.ModuleDict({
            'shared_text': self._create_decoder(base_model),
            'qa_generation': self._create_decoder(base_model),
        })
        
        # Language Model Heads (ê·¸ë£¹ë³„)
        vocab_size = base_model.config.vocab_size
        hidden_size = base_model.config.d_model
        
        self.lm_heads = nn.ModuleDict({
            'shared_text': nn.Linear(hidden_size, vocab_size, bias=False),
            'qa_generation': nn.Linear(hidden_size, vocab_size, bias=False),
        })
        
        print("[OK] ë””ì½”ë” í—¤ë“œ êµ¬ì„± ì™„ë£Œ:")
        print("  - Shared Text Decoder: style/dialogue/role")
        print("  - QA Decoder: qa_generation")
        
        self.config = base_model.config
        
    def _create_decoder(self, base_model):
        """ê¸°ë³¸ ëª¨ë¸ì˜ ë””ì½”ë”ë¥¼ ë³µì‚¬í•˜ì—¬ ìƒˆë¡œìš´ ë””ì½”ë” ìƒì„±"""
        decoder = copy.deepcopy(base_model.model.decoder)
        if self.gradient_checkpointing and hasattr(decoder, "gradient_checkpointing"):
            decoder.gradient_checkpointing = True
        return decoder

    @staticmethod
    def _shrink_ffn_layer(layer: nn.Module, target_dim: int):
        if not hasattr(layer, "fc1") or not hasattr(layer, "fc2"):
            return
        fc1: nn.Linear = layer.fc1
        fc2: nn.Linear = layer.fc2
        current_dim = fc1.out_features
        if target_dim >= current_dim:
            return

        new_fc1 = nn.Linear(fc1.in_features, target_dim, bias=fc1.bias is not None)
        with torch.no_grad():
            new_fc1.weight.copy_(fc1.weight[:target_dim, :])
            if fc1.bias is not None:
                new_fc1.bias.copy_(fc1.bias[:target_dim])
        layer.fc1 = new_fc1

        new_fc2 = nn.Linear(target_dim, fc2.out_features, bias=fc2.bias is not None)
        with torch.no_grad():
            new_fc2.weight.copy_(fc2.weight[:, :target_dim])
            if fc2.bias is not None:
                new_fc2.bias.copy_(fc2.bias)
        layer.fc2 = new_fc2

    def _apply_ffn_reduction(self, base_model: BartForConditionalGeneration, target_dim: int):
        for enc_layer in base_model.model.encoder.layers:
            self._shrink_ffn_layer(enc_layer, target_dim)
        for dec_layer in base_model.model.decoder.layers:
            self._shrink_ffn_layer(dec_layer, target_dim)
    
    def forward(
        self,
        input_ids: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        decoder_input_ids: Optional[torch.Tensor] = None,
        task: str = 'style_transfer'
    ):
        """
        Forward pass
        
        Args:
            input_ids: ì…ë ¥ í† í° IDs
            attention_mask: ì–´í…ì…˜ ë§ˆìŠ¤í¬
            decoder_input_ids: ë””ì½”ë” ì…ë ¥ IDs
            task: íƒœìŠ¤í¬ ì´ë¦„ ('style_transfer', 'dialogue_summarization', 
                              'role_generation', 'qa_generation')
        """
        # Shared Encoderë¡œ ì¸ì½”ë”©
        encoder_outputs = self.shared_encoder(
            input_ids=input_ids,
            attention_mask=attention_mask
        )
        
        # Task-specific Decoderë¡œ ë””ì½”ë”©
        if task not in self.task_to_decoder:
            raise ValueError(f"Unknown task: {task}")
        
        decoder_key = self.task_to_decoder[task]
        decoder = self.decoders[decoder_key]
        lm_head = self.lm_heads[decoder_key]
        
        decoder_outputs = decoder(
            input_ids=decoder_input_ids,
            encoder_hidden_states=encoder_outputs.last_hidden_state,
            encoder_attention_mask=attention_mask
        )
        
        # LM Headë¥¼ í†µí•´ ìµœì¢… logits ìƒì„±
        logits = lm_head(decoder_outputs.last_hidden_state)
        
        return {
            'logits': logits,
            'encoder_outputs': encoder_outputs,
            'decoder_outputs': decoder_outputs
        }
    
    def generate(
        self,
        input_ids: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        task: str = 'style_transfer',
        max_length: int = 50,
        num_beams: int = 5, # êµ¬í˜„ í•„ìš”
        **kwargs
    ):
        """
        í…ìŠ¤íŠ¸ ìƒì„±
        
        Args:
            input_ids: ì…ë ¥ í† í° IDs
            attention_mask: ì–´í…ì…˜ ë§ˆìŠ¤í¬
            task: íƒœìŠ¤í¬ ì´ë¦„
            max_length: ìµœëŒ€ ìƒì„± ê¸¸ì´
            num_beams: Beam search í¬ê¸°
        """
        # Shared Encoderë¡œ ì¸ì½”ë”©
        encoder_outputs = self.shared_encoder(
            input_ids=input_ids,
            attention_mask=attention_mask
        )
        
        # Task-specific Decoder ì„ íƒ
        decoder_key = self.task_to_decoder.get(task)
        if decoder_key is None:
            raise ValueError(f"Unknown task: {task}")
        decoder = self.decoders[decoder_key]
        lm_head = self.lm_heads[decoder_key]
        
        # ê°„ë‹¨í•œ greedy decoding (ì‹¤ì œë¡œëŠ” beam search êµ¬í˜„ í•„ìš”)
        batch_size = input_ids.size(0)
        device = input_ids.device
        
        # ì‹œì‘ í† í° (BOS)
        decoder_input_ids = torch.full(
            (batch_size, 1),
            self.config.decoder_start_token_id,
            dtype=torch.long,
            device=device
        )
        
        generated = decoder_input_ids
        
        for _ in range(max_length):
            decoder_outputs = decoder(
                input_ids=generated,
                encoder_hidden_states=encoder_outputs.last_hidden_state,
                encoder_attention_mask=attention_mask
            )
            
            logits = lm_head(decoder_outputs.last_hidden_state)
            next_token_logits = logits[:, -1, :]
            next_tokens = torch.argmax(next_token_logits, dim=-1, keepdim=True)
            
            generated = torch.cat([generated, next_tokens], dim=1)
            
            # EOS í† í°ì´ë©´ ì¢…ë£Œ
            if (next_tokens == self.config.eos_token_id).all():
                break
        
        return generated
    
    def get_encoder_parameters(self):
        """ê³µìœ  ì¸ì½”ë”ì˜ íŒŒë¼ë¯¸í„° ë°˜í™˜"""
        return self.shared_encoder.parameters()
    
    def get_decoder_parameters(self, task: str):
        """íŠ¹ì • íƒœìŠ¤í¬ ë””ì½”ë”ì˜ íŒŒë¼ë¯¸í„° ë°˜í™˜"""
        if task not in self.task_to_decoder:
            raise ValueError(f"Unknown task: {task}")
        decoder_key = self.task_to_decoder[task]
        return list(self.decoders[decoder_key].parameters()) + list(self.lm_heads[decoder_key].parameters())
    
    def freeze_encoder(self):
        """ì¸ì½”ë” íŒŒë¼ë¯¸í„° ê³ ì •"""
        for param in self.shared_encoder.parameters():
            param.requires_grad = False
        print("[OK] Encoder íŒŒë¼ë¯¸í„° ê³ ì •")
    
    def unfreeze_encoder(self):
        """ì¸ì½”ë” íŒŒë¼ë¯¸í„° í•´ì œ"""
        for param in self.shared_encoder.parameters():
            param.requires_grad = True
        print("[OK] Encoder íŒŒë¼ë¯¸í„° í•™ìŠµ ê°€ëŠ¥")


def main():
    """í…ŒìŠ¤íŠ¸ ë° ì‚¬ìš© ì˜ˆì œ"""
    print("="*60)
    print("Multi-Task KoBART ëª¨ë¸ í…ŒìŠ¤íŠ¸")
    print("="*60)
    print()
    
    # ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
    tokenizer = PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v1')
    model = MultiTaskKoBART()
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.to(device)
    model.eval()
    
    print(f"\n[OK] ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (ë””ë°”ì´ìŠ¤: {device})")
    
    # ëª¨ë¸ ì •ë³´ ì¶œë ¥
    total_params = sum(p.numel() for p in model.parameters())
    encoder_params = sum(p.numel() for p in model.shared_encoder.parameters())
    
    print(f"\nëª¨ë¸ ì •ë³´:")
    print(f"  - ì „ì²´ íŒŒë¼ë¯¸í„°: {total_params:,}")
    print(f"  - ê³µìœ  ì¸ì½”ë” íŒŒë¼ë¯¸í„°: {encoder_params:,}")
    print(f"  - ë””ì½”ë” í—¤ë“œ ê°œìˆ˜: 4ê°œ")
    
    # ê° íƒœìŠ¤í¬ë³„ í…ŒìŠ¤íŠ¸
    test_cases = {
        'style_transfer': "ì´ ë¬¸ì¥ì„ ê²©ì‹ìˆëŠ” í‘œí˜„ìœ¼ë¡œ ë°”ê¿”ì£¼ì„¸ìš”.",
        'dialogue_summarization': "A: ì•ˆë…•í•˜ì„¸ìš”. B: ë„¤, ì•ˆë…•í•˜ì„¸ìš”. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?",
        'role_generation': "ì„ ìƒë‹˜ ì—­í• ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”: ì¸ê³µì§€ëŠ¥ì´ë€ ë¬´ì—‡ì¸ê°€ìš”?",
        'qa_generation': "ì§ˆë¬¸: í•œêµ­ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì¸ê°€ìš”?"
    }
    
    print("\n" + "="*60)
    print("íƒœìŠ¤í¬ë³„ ìƒì„± í…ŒìŠ¤íŠ¸")
    print("="*60)
    
    with torch.no_grad():
        for task, text in test_cases.items():
            print(f"\n[{task.upper()}]")
            print(f"ì…ë ¥: {text}")
            
            # í† í°í™”
            inputs = tokenizer(text, return_tensors="pt", max_length=512, truncation=True)
            inputs = {k: v.to(device) for k, v in inputs.items()}
            
            # ìƒì„±
            try:
                outputs = model.generate(
                    input_ids=inputs['input_ids'],
                    attention_mask=inputs['attention_mask'],
                    task=task,
                    max_length=50
                )
                
                # ë””ì½”ë”©
                result = tokenizer.decode(outputs[0], skip_special_tokens=True)
                print(f"ì¶œë ¥: {result}")
            except Exception as e:
                print(f"ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
    
    print("\n" + "="*60)
    print("âœ“ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("="*60)
    
    # ì‚¬ìš© íŒ
    print("\nğŸ’¡ ì‚¬ìš© íŒ:")
    print("1. ê° íƒœìŠ¤í¬ë³„ë¡œ ë…ë¦½ì ìœ¼ë¡œ fine-tuning ê°€ëŠ¥")
    print("2. ì¸ì½”ë”ëŠ” ëª¨ë“  íƒœìŠ¤í¬ì—ì„œ ê³µìœ ë˜ì–´ íš¨ìœ¨ì ")
    print("3. model.freeze_encoder()ë¡œ ì¸ì½”ë” ê³ ì • ê°€ëŠ¥")
    print("4. íƒœìŠ¤í¬ë³„ í•™ìŠµ ë°ì´í„°ë¡œ ê° ë””ì½”ë” í—¤ë“œ í•™ìŠµ")


if __name__ == "__main__":
    main()

