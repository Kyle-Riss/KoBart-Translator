"""
Multi-Task KoBART Architecture
- Shared Encoder (1ê°œ)
- 4ê°œì˜ Task-specific Decoder Heads
"""

import torch
import torch.nn as nn
from transformers import BartForConditionalGeneration, PreTrainedTokenizerFast
from typing import Dict, Optional


class MultiTaskKoBART(nn.Module):
    """
    KoBART ê¸°ë°˜ ë©€í‹°íƒœìŠ¤í¬ ëª¨ë¸
    
    êµ¬ì¡°:
    - Shared Encoder (KoBART ì¸ì½”ë”)
    - 4ê°œì˜ Decoder Heads:
        1. Style Transfer
        2. Dialogue Summarization
        3. Role-conditioned Generation
        4. QA Answer Generation
    """
    
    def __init__(self, model_name: str = 'gogamza/kobart-base-v1'):
        super(MultiTaskKoBART, self).__init__()
        
        print("Multi-Task KoBART ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
        
        # ê¸°ë³¸ KoBART ëª¨ë¸ ë¡œë“œ
        base_model = BartForConditionalGeneration.from_pretrained(model_name)
        
        # Shared Encoder (ê³µìœ  ì¸ì½”ë”)
        self.shared_encoder = base_model.model.encoder
        print("[OK] Shared Encoder ë¡œë“œ ì™„ë£Œ")
        
        # ê¸°ë³¸ ë””ì½”ë” ì„¤ì • ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        decoder_config = base_model.model.decoder.config
        
        # 4ê°œì˜ Task-specific Decoder Heads
        # ê° ë””ì½”ë”ëŠ” ë™ì¼í•œ êµ¬ì¡°ë¥¼ ê°€ì§€ì§€ë§Œ ë…ë¦½ì ìœ¼ë¡œ í•™ìŠµë¨
        self.decoders = nn.ModuleDict({
            'style_transfer': self._create_decoder(base_model),
            'dialogue_summarization': self._create_decoder(base_model),
            'role_generation': self._create_decoder(base_model),
            'qa_generation': self._create_decoder(base_model)
        })
        
        # Language Model Heads (ê° ë””ì½”ë”ìš©)
        vocab_size = base_model.config.vocab_size
        hidden_size = base_model.config.d_model
        
        self.lm_heads = nn.ModuleDict({
            'style_transfer': nn.Linear(hidden_size, vocab_size, bias=False),
            'dialogue_summarization': nn.Linear(hidden_size, vocab_size, bias=False),
            'role_generation': nn.Linear(hidden_size, vocab_size, bias=False),
            'qa_generation': nn.Linear(hidden_size, vocab_size, bias=False)
        })
        
        print("[OK] 4ê°œì˜ Decoder Heads ìƒì„± ì™„ë£Œ:")
        print("  - Head 1: Style Transfer")
        print("  - Head 2: Dialogue Summarization")
        print("  - Head 3: Role-conditioned Generation")
        print("  - Head 4: QA Answer Generation")
        
        self.config = base_model.config
        
    def _create_decoder(self, base_model):
        """ê¸°ë³¸ ëª¨ë¸ì˜ ë””ì½”ë”ë¥¼ ë³µì‚¬í•˜ì—¬ ìƒˆë¡œìš´ ë””ì½”ë” ìƒì„±"""
        import copy
        return copy.deepcopy(base_model.model.decoder)
    
    def forward(
        self,
        input_ids: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        decoder_input_ids: Optional[torch.Tensor] = None,
        task: str = 'style_transfer'
    ):
        """
        Forward pass
        
        Args:
            input_ids: ì…ë ¥ í† í° IDs
            attention_mask: ì–´í…ì…˜ ë§ˆìŠ¤í¬
            decoder_input_ids: ë””ì½”ë” ì…ë ¥ IDs
            task: íƒœìŠ¤í¬ ì´ë¦„ ('style_transfer', 'dialogue_summarization', 
                              'role_generation', 'qa_generation')
        """
        # Shared Encoderë¡œ ì¸ì½”ë”©
        encoder_outputs = self.shared_encoder(
            input_ids=input_ids,
            attention_mask=attention_mask
        )
        
        # Task-specific Decoderë¡œ ë””ì½”ë”©
        if task not in self.decoders:
            raise ValueError(f"Unknown task: {task}")
        
        decoder = self.decoders[task]
        lm_head = self.lm_heads[task]
        
        decoder_outputs = decoder(
            input_ids=decoder_input_ids,
            encoder_hidden_states=encoder_outputs.last_hidden_state,
            encoder_attention_mask=attention_mask
        )
        
        # LM Headë¥¼ í†µí•´ ìµœì¢… logits ìƒì„±
        logits = lm_head(decoder_outputs.last_hidden_state)
        
        return {
            'logits': logits,
            'encoder_outputs': encoder_outputs,
            'decoder_outputs': decoder_outputs
        }
    
    def generate(
        self,
        input_ids: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        task: str = 'style_transfer',
        max_length: int = 50,
        num_beams: int = 5, # êµ¬í˜„ í•„ìš”
        **kwargs
    ):
        """
        í…ìŠ¤íŠ¸ ìƒì„±
        
        Args:
            input_ids: ì…ë ¥ í† í° IDs
            attention_mask: ì–´í…ì…˜ ë§ˆìŠ¤í¬
            task: íƒœìŠ¤í¬ ì´ë¦„
            max_length: ìµœëŒ€ ìƒì„± ê¸¸ì´
            num_beams: Beam search í¬ê¸°
        """
        # Shared Encoderë¡œ ì¸ì½”ë”©
        encoder_outputs = self.shared_encoder(
            input_ids=input_ids,
            attention_mask=attention_mask
        )
        
        # Task-specific Decoder ì„ íƒ
        decoder = self.decoders[task]
        lm_head = self.lm_heads[task]
        
        # ê°„ë‹¨í•œ greedy decoding (ì‹¤ì œë¡œëŠ” beam search êµ¬í˜„ í•„ìš”)
        batch_size = input_ids.size(0)
        device = input_ids.device
        
        # ì‹œì‘ í† í° (BOS)
        decoder_input_ids = torch.full(
            (batch_size, 1),
            self.config.decoder_start_token_id,
            dtype=torch.long,
            device=device
        )
        
        generated = decoder_input_ids
        
        for _ in range(max_length):
            decoder_outputs = decoder(
                input_ids=generated,
                encoder_hidden_states=encoder_outputs.last_hidden_state,
                encoder_attention_mask=attention_mask
            )
            
            logits = lm_head(decoder_outputs.last_hidden_state)
            next_token_logits = logits[:, -1, :]
            next_tokens = torch.argmax(next_token_logits, dim=-1, keepdim=True)
            
            generated = torch.cat([generated, next_tokens], dim=1)
            
            # EOS í† í°ì´ë©´ ì¢…ë£Œ
            if (next_tokens == self.config.eos_token_id).all():
                break
        
        return generated
    
    def get_encoder_parameters(self):
        """ê³µìœ  ì¸ì½”ë”ì˜ íŒŒë¼ë¯¸í„° ë°˜í™˜"""
        return self.shared_encoder.parameters()
    
    def get_decoder_parameters(self, task: str):
        """íŠ¹ì • íƒœìŠ¤í¬ ë””ì½”ë”ì˜ íŒŒë¼ë¯¸í„° ë°˜í™˜"""
        if task not in self.decoders:
            raise ValueError(f"Unknown task: {task}")
        return list(self.decoders[task].parameters()) + list(self.lm_heads[task].parameters())
    
    def freeze_encoder(self):
        """ì¸ì½”ë” íŒŒë¼ë¯¸í„° ê³ ì •"""
        for param in self.shared_encoder.parameters():
            param.requires_grad = False
        print("[OK] Encoder íŒŒë¼ë¯¸í„° ê³ ì •")
    
    def unfreeze_encoder(self):
        """ì¸ì½”ë” íŒŒë¼ë¯¸í„° í•´ì œ"""
        for param in self.shared_encoder.parameters():
            param.requires_grad = True
        print("[OK] Encoder íŒŒë¼ë¯¸í„° í•™ìŠµ ê°€ëŠ¥")


def main():
    """í…ŒìŠ¤íŠ¸ ë° ì‚¬ìš© ì˜ˆì œ"""
    print("="*60)
    print("Multi-Task KoBART ëª¨ë¸ í…ŒìŠ¤íŠ¸")
    print("="*60)
    print()
    
    # ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
    tokenizer = PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v1')
    model = MultiTaskKoBART()
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.to(device)
    model.eval()
    
    print(f"\n[OK] ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (ë””ë°”ì´ìŠ¤: {device})")
    
    # ëª¨ë¸ ì •ë³´ ì¶œë ¥
    total_params = sum(p.numel() for p in model.parameters())
    encoder_params = sum(p.numel() for p in model.shared_encoder.parameters())
    
    print(f"\nëª¨ë¸ ì •ë³´:")
    print(f"  - ì „ì²´ íŒŒë¼ë¯¸í„°: {total_params:,}")
    print(f"  - ê³µìœ  ì¸ì½”ë” íŒŒë¼ë¯¸í„°: {encoder_params:,}")
    print(f"  - ë””ì½”ë” í—¤ë“œ ê°œìˆ˜: 4ê°œ")
    
    # ê° íƒœìŠ¤í¬ë³„ í…ŒìŠ¤íŠ¸
    test_cases = {
        'style_transfer': "ì´ ë¬¸ì¥ì„ ê²©ì‹ìˆëŠ” í‘œí˜„ìœ¼ë¡œ ë°”ê¿”ì£¼ì„¸ìš”.",
        'dialogue_summarization': "A: ì•ˆë…•í•˜ì„¸ìš”. B: ë„¤, ì•ˆë…•í•˜ì„¸ìš”. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?",
        'role_generation': "ì„ ìƒë‹˜ ì—­í• ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”: ì¸ê³µì§€ëŠ¥ì´ë€ ë¬´ì—‡ì¸ê°€ìš”?",
        'qa_generation': "ì§ˆë¬¸: í•œêµ­ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì¸ê°€ìš”?"
    }
    
    print("\n" + "="*60)
    print("íƒœìŠ¤í¬ë³„ ìƒì„± í…ŒìŠ¤íŠ¸")
    print("="*60)
    
    with torch.no_grad():
        for task, text in test_cases.items():
            print(f"\n[{task.upper()}]")
            print(f"ì…ë ¥: {text}")
            
            # í† í°í™”
            inputs = tokenizer(text, return_tensors="pt", max_length=512, truncation=True)
            inputs = {k: v.to(device) for k, v in inputs.items()}
            
            # ìƒì„±
            try:
                outputs = model.generate(
                    input_ids=inputs['input_ids'],
                    attention_mask=inputs['attention_mask'],
                    task=task,
                    max_length=50
                )
                
                # ë””ì½”ë”©
                result = tokenizer.decode(outputs[0], skip_special_tokens=True)
                print(f"ì¶œë ¥: {result}")
            except Exception as e:
                print(f"ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
    
    print("\n" + "="*60)
    print("âœ“ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("="*60)
    
    # ì‚¬ìš© íŒ
    print("\nğŸ’¡ ì‚¬ìš© íŒ:")
    print("1. ê° íƒœìŠ¤í¬ë³„ë¡œ ë…ë¦½ì ìœ¼ë¡œ fine-tuning ê°€ëŠ¥")
    print("2. ì¸ì½”ë”ëŠ” ëª¨ë“  íƒœìŠ¤í¬ì—ì„œ ê³µìœ ë˜ì–´ íš¨ìœ¨ì ")
    print("3. model.freeze_encoder()ë¡œ ì¸ì½”ë” ê³ ì • ê°€ëŠ¥")
    print("4. íƒœìŠ¤í¬ë³„ í•™ìŠµ ë°ì´í„°ë¡œ ê° ë””ì½”ë” í—¤ë“œ í•™ìŠµ")


if __name__ == "__main__":
    main()

