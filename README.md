# KoBART Multi-Task Learning Project

KoBARTëŠ” í•œêµ­ì–´ì— íŠ¹í™”ëœ BART (Bidirectional and Auto-Regressive Transformers) ëª¨ë¸ì…ë‹ˆë‹¤.

ì´ í”„ë¡œì íŠ¸ëŠ” **í•˜ë‚˜ì˜ ê³µìœ  ì¸ì½”ë”**ì™€ **4ê°œì˜ íƒœìŠ¤í¬ë³„ ë””ì½”ë” í—¤ë“œ**ë¥¼ ê°€ì§„ ë©€í‹°íƒœìŠ¤í¬ í•™ìŠµ ì•„í‚¤í…ì²˜ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.

## ğŸ¯ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
ì…ë ¥ â†’ Shared Encoder â†’ 4ê°œì˜ Decoder Heads
                         â”œâ”€â”€ Style Transfer
                         â”œâ”€â”€ Dialogue Summarization  
                         â”œâ”€â”€ Role-based Generation
                         â””â”€â”€ QA Answer Generation
```

## ğŸ“ í´ë” êµ¬ì„±

- `kobart_translator/`: MultiTaskKoBART ë° ë°ì´í„° ë¡œë” ë“± í•µì‹¬ ëª¨ë“ˆ
- `scripts/basic|demos|training|data/`: ì‹¤í–‰Â·ë°ëª¨Â·í•™ìŠµÂ·ë°ì´í„° ìœ í‹¸ ìŠ¤í¬ë¦½íŠ¸
- `tests/`: íšŒê·€ ë° ìœ ë‹› í…ŒìŠ¤íŠ¸
- `docs/`: ì•„í‚¤í…ì²˜ ë° ì‚¬ìš© ê°€ì´ë“œ ë¬¸ì„œ
- `data/`: íƒœìŠ¤í¬ë³„ ë°ì´í„°ì…‹ (ì•„ë˜ ì°¸ì¡°)
- `logs/`: í•™ìŠµ/ì‹¤í–‰ ë¡œê·¸

### ë°ì´í„°ì…‹ ë””ë ‰í„°ë¦¬

| Task | Path | ë¹„ê³  |
| --- | --- | --- |
| Style Transfer | `data/style_transfer/korean_smile_style_dataset/` | Smilegate submodule |
| Dialogue Summarization | `data/dialogue_summarization/aihub_dialogue_summary/` | AI Hub ì›ë³¸ ZIP |
| Role-conditioned Generation | `data/role_generation/aihub_dialogue_role_dataset/` | AI Hub ì‘ê¸‰/ì˜¤í”¼ìŠ¤ ëŒ€í™” ZIP |
| QA Answer Generation | `data/qa/korquad/` | `scripts/data/download_korquad.py`ë¡œ KorQuAD1 ì €ì¥, 2.0ì€ ìˆ˜ë™ ì¶”ê°€ |

```bash
# KorQuAD1 train/dev JSON ìë™ ì¶”ì¶œ
python scripts/data/download_korquad.py --output data/qa/korquad

# íƒœìŠ¤í¬ë³„ JSONL ìƒì„± (style/dialogue/role/qa)
python scripts/data/prepare_multitask_dataset.py --output-dir data/processed
```

## ì„¤ì¹˜ ë°©ë²•

### 1. í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜

```bash
pip install -r requirements.txt
```

ë˜ëŠ” ê°œë³„ ì„¤ì¹˜:

```bash
pip install torch transformers sentencepiece
```

## ì‚¬ìš© ë°©ë²•

### 1. ê¸°ë³¸ KoBART ëª¨ë¸

#### ê¸°ë³¸ ëª¨ë¸ ë¡œë“œ ë° í…ŒìŠ¤íŠ¸

```bash
python3 scripts/basic/quick_start.py           # ë¹ ë¥¸ ì‹œì‘
python3 scripts/basic/example_simple.py        # ìƒì„¸ ì˜ˆì œ
python3 scripts/basic/verify_installation.py   # ì„¤ì¹˜ ê²€ì¦
python3 scripts/basic/load_kobart.py           # ëŒ€í™”í˜• ëª¨ë“œ í¬í•¨
python3 scripts/demos/interactive_demo.py      # ì‹¤ì‹œê°„ ë°ëª¨
```

### 2. Multi-Task KoBART ëª¨ë¸

#### ëª¨ë¸ í…ŒìŠ¤íŠ¸

```bash
python3 -m kobart_translator.multi_task
```

#### í•™ìŠµ ì‹œì‘

```bash
python3 scripts/training/train_multi_task.py
```

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ë‹¤ìŒ ì‘ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤:
- ê³µìœ  ì¸ì½”ë” ë¡œë“œ
- 4ê°œì˜ íƒœìŠ¤í¬ë³„ ë””ì½”ë” ìƒì„±
- ìƒ˜í”Œ ë°ì´í„°ë¡œ í•™ìŠµ

### Python ì½”ë“œì—ì„œ ì§ì ‘ ì‚¬ìš©

```python
from transformers import BartForConditionalGeneration, PreTrainedTokenizerFast

# ëª¨ë¸ ë¡œë“œ
tokenizer = PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v1')
model = BartForConditionalGeneration.from_pretrained('gogamza/kobart-base-v1')

# í…ìŠ¤íŠ¸ ìƒì„±
text = "KoBARTëŠ” í•œêµ­ì–´ì— íŠ¹í™”ëœ BART ëª¨ë¸ì…ë‹ˆë‹¤."
inputs = tokenizer(text, return_tensors="pt")
output_ids = model.generate(inputs['input_ids'], max_length=50)
output = tokenizer.decode(output_ids[0], skip_special_tokens=True)

print(output)
```

## ì£¼ìš” ê¸°ëŠ¥

### ê¸°ë³¸ KoBART
- **ìš”ì•½ ìƒì„±**: ê¸´ í…ìŠ¤íŠ¸ë¥¼ ìš”ì•½
- **í…ìŠ¤íŠ¸ ìƒì„±**: ì£¼ì–´ì§„ í”„ë¡¬í”„íŠ¸ë¡œë¶€í„° í…ìŠ¤íŠ¸ ìƒì„±
- **ë¬¸ì¥ ë³€í™˜**: ë¬¸ì¥ì„ ë‹¤ë¥¸ í˜•íƒœë¡œ ë³€í™˜

### Multi-Task KoBART (4ê°œì˜ ì „ë¬¸ ë””ì½”ë”)
1. **Style Transfer**: êµ¬ì–´ì²´ â†” ê²©ì‹ì²´ ë³€í™˜
2. **Dialogue Summarization**: ëŒ€í™” ë‚´ìš© ìš”ì•½
3. **Role-conditioned Generation**: ì—­í•  ê¸°ë°˜ ì‘ë‹µ ìƒì„± (ì„ ìƒë‹˜, ì¹œêµ¬ ë“±)
4. **QA Answer Generation**: ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„±

## ëª¨ë¸ ì •ë³´

- **ëª¨ë¸ëª…**: gogamza/kobart-base-v1
- **ê¸°ë°˜**: BART (Facebook AI)
- **ì–¸ì–´**: í•œêµ­ì–´
- **íƒœìŠ¤í¬**: ìš”ì•½, ìƒì„±, ë³€í™˜ ë“±

## ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­

- Python 3.8 ì´ìƒ
- PyTorch 2.0 ì´ìƒ
- ìµœì†Œ 8GB RAM ê¶Œì¥
- GPU ì‚¬ìš© ì‹œ ë” ë¹ ë¥¸ ì²˜ë¦¬ ê°€ëŠ¥ (ì„ íƒì‚¬í•­)

## ğŸ“š ë¬¸ì„œ

- **docs/MULTI_TASK_GUIDE.md**: ë©€í‹°íƒœìŠ¤í¬ ì‚¬ìš© ê°€ì´ë“œ
- **docs/ARCHITECTURE.md**: ì•„í‚¤í…ì²˜ ìƒì„¸ ì„¤ëª…
- **docs/USAGE_GUIDE.md**: ê¸°ë³¸ ì‚¬ìš©ë²•

## ğŸ“Š ëª¨ë¸ ì •ë³´

### ê¸°ë³¸ KoBART
- íŒŒë¼ë¯¸í„°: ~124M

### Multi-Task KoBART
- ê³µìœ  ì¸ì½”ë”: ~66M íŒŒë¼ë¯¸í„°
- 4ê°œ ë””ì½”ë”: ê° ~103M íŒŒë¼ë¯¸í„°
- ì´ íŒŒë¼ë¯¸í„°: ~481M

### Tiny Student Model (Knowledge Distillation)
- ì´ˆê²½ëŸ‰ ëª¨ë¸: ~1M~5M íŒŒë¼ë¯¸í„°
- Teacher ëª¨ë¸ë¡œë¶€í„° ì§€ì‹ ì¦ë¥˜
- ìì„¸í•œ ë‚´ìš©ì€ `docs/STUDENT_DISTILLATION_PLAN.md` ì°¸ì¡°

## ğŸš€ ìµœê·¼ ê°œì„ ì‚¬í•­

### í•™ìŠµ ê°œì„ 
- **ì •ê·œí™”**: Weight decay (ê¸°ë³¸ê°’ 0.01) ì¶”ê°€ë¡œ ê³¼ì í•© ë°©ì§€
- **Early Stopping**: ê²€ì¦ ì†ì‹¤ì´ ê°œì„ ë˜ì§€ ì•Šìœ¼ë©´ ìë™ìœ¼ë¡œ í•™ìŠµ ì¤‘ë‹¨
- **Learning Rate Scheduler**: ReduceLROnPlateau ë˜ëŠ” CosineAnnealingLR ì§€ì›
- **Mixed Precision Training**: AMP ì§€ì›ìœ¼ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± í–¥ìƒ
- **Gradient Checkpointing**: ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°ì†Œ ì˜µì…˜

### ì‹œê°í™” ë„êµ¬
- í•™ìŠµ ê³¡ì„  ì‹œê°í™”: `scripts/utils/plot_training.py`
- íƒœìŠ¤í¬ë³„ ì†ì‹¤ ì¶”ì  ë° ë¹„êµ
- TensorBoard ë¡œê¹… ì§€ì›

### Knowledge Distillation
- Tiny Student ëª¨ë¸ êµ¬í˜„ (`kobart_translator/tiny_student.py`)
- Teacher-Student í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ (`scripts/training/train_student.py`)
- 8K SentencePiece í† í¬ë‚˜ì´ì € ì¬í•™ìŠµ ì§€ì›

## ì°¸ê³  ìë£Œ

- [Hugging Face Model Hub](https://huggingface.co/gogamza/kobart-base-v1)
- [BART ë…¼ë¬¸](https://arxiv.org/abs/1910.13461)


