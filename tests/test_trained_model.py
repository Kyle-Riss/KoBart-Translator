"""
í•™ìŠµëœ Multi-Task KoBART ëª¨ë¸ í…ŒìŠ¤íŠ¸
"""

import torch
from kobart_translator import MultiTaskKoBART
from transformers import PreTrainedTokenizerFast


def load_trained_model(checkpoint_path: str):
    """í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ"""
    print("í•™ìŠµëœ ëª¨ë¸ ë¡œë”© ì¤‘...")
    
    model = MultiTaskKoBART()
    
    # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
    checkpoint = torch.load(checkpoint_path, map_location='cpu')
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()
    
    print("âœ“ í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    
    return model


def test_task(model, tokenizer, task, test_cases, device):
    """íŠ¹ì • íƒœìŠ¤í¬ í…ŒìŠ¤íŠ¸"""
    print(f"\n{'='*60}")
    print(f"[{task.upper().replace('_', ' ')}]")
    print('='*60)
    
    with torch.no_grad():
        for i, (input_text, expected) in enumerate(test_cases, 1):
            print(f"\ní…ŒìŠ¤íŠ¸ {i}:")
            print(f"ì…ë ¥: {input_text}")
            if expected:
                print(f"ê¸°ëŒ€ê°’: {expected}")
            
            # í† í°í™”
            inputs = tokenizer(input_text, return_tensors="pt", max_length=512, truncation=True)
            inputs = {k: v.to(device) for k, v in inputs.items()}
            
            # ìƒì„±
            try:
                outputs = model.generate(
                    input_ids=inputs['input_ids'],
                    attention_mask=inputs['attention_mask'],
                    task=task,
                    max_length=100
                )
                
                # ë””ì½”ë”©
                result = tokenizer.decode(outputs[0], skip_special_tokens=True)
                print(f"ì¶œë ¥: {result}")
                
            except Exception as e:
                print(f"ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("="*60)
    print("í•™ìŠµëœ Multi-Task KoBART ëª¨ë¸ í…ŒìŠ¤íŠ¸")
    print("="*60)
    print()
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ë””ë°”ì´ìŠ¤: {device}\n")
    
    # í† í¬ë‚˜ì´ì € ë¡œë“œ
    print("í† í¬ë‚˜ì´ì € ë¡œë”© ì¤‘...")
    tokenizer = PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v1')
    print("âœ“ í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ\n")
    
    # í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ
    checkpoint_path = "/Users/arka/Desktop/Ko-bart/multi_task_model.pt"
    model = load_trained_model(checkpoint_path)
    model.to(device)
    
    print(f"\nëª¨ë¸ ì •ë³´:")
    total_params = sum(p.numel() for p in model.parameters())
    print(f"  - ì „ì²´ íŒŒë¼ë¯¸í„°: {total_params:,}")
    print(f"  - ë””ë°”ì´ìŠ¤: {device}")
    
    # íƒœìŠ¤í¬ë³„ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤
    test_data = {
        'style_transfer': [
            ("ì´ê±° ì¢€ ë„ì™€ì£¼ì„¸ìš”.", "ì´ê²ƒì„ ë„ì™€ì£¼ì‹œê² ìŠµë‹ˆê¹Œ?"),
            ("ë¹¨ë¦¬ ì™€.", "ë¹ ë¥¸ ì‹œì¼ ë‚´ì— ë°©ë¬¸í•´ ì£¼ì‹œê¸° ë°”ëë‹ˆë‹¤."),
            ("ë­í•´?", "ë¬´ì—‡ì„ í•˜ê³  ê³„ì‹­ë‹ˆê¹Œ?"),
            ("ê³ ë§ˆì›Œ.", "ê°ì‚¬í•©ë‹ˆë‹¤."),
        ],
        
        'dialogue_summarization': [
            ("A: ë‚´ì¼ íšŒì˜ ëª‡ ì‹œì—ìš”? B: ì˜¤í›„ 2ì‹œì…ë‹ˆë‹¤. A: ì•Œê² ìŠµë‹ˆë‹¤.", "ë‚´ì¼ íšŒì˜ëŠ” ì˜¤í›„ 2ì‹œì…ë‹ˆë‹¤."),
            ("A: ì ì‹¬ ë­ ë¨¹ì„ê¹Œìš”? B: í•œì‹ì´ ì¢‹ê² ì–´ìš”. A: ì¢‹ì•„ìš”.", "ì ì‹¬ìœ¼ë¡œ í•œì‹ì„ ë¨¹ê¸°ë¡œ í–ˆìŠµë‹ˆë‹¤."),
            ("A: ì˜¤ëŠ˜ ë‚ ì”¨ ì–´ë•Œ? B: ë§‘ê³  ì¢‹ì•„ìš”.", "ì˜¤ëŠ˜ ë‚ ì”¨ëŠ” ë§‘ê³  ì¢‹ìŠµë‹ˆë‹¤."),
        ],
        
        'role_generation': [
            ("[ì„ ìƒë‹˜] íŒŒì´ì¬ì´ë€ ë¬´ì—‡ì¸ê°€ìš”?", "íŒŒì´ì¬ì€ ë°°ìš°ê¸° ì‰¬ìš´ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤."),
            ("[ì¹œêµ¬] ì£¼ë§ì— ë­í•´?", "íŠ¹ë³„í•œ ê³„íšì€ ì—†ì–´. ë„ˆëŠ”?"),
            ("[ì„ ìƒë‹˜] ì¸ê³µì§€ëŠ¥ì˜ ì¥ì ì€?", "ì¸ê³µì§€ëŠ¥ì€ ëŒ€ëŸ‰ì˜ ë°ì´í„°ë¥¼ ë¹ ë¥´ê²Œ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."),
        ],
        
        'qa_generation': [
            ("ì§ˆë¬¸: ì„œìš¸ì˜ ì¸êµ¬ëŠ” ì–¼ë§ˆë‚˜ ë˜ë‚˜ìš”?", "ì„œìš¸ì˜ ì¸êµ¬ëŠ” ì•½ 1ì²œë§Œ ëª…ì…ë‹ˆë‹¤."),
            ("ì§ˆë¬¸: ì¸ê³µì§€ëŠ¥ì˜ ì¥ì ì€ ë¬´ì—‡ì¸ê°€ìš”?", "ì¸ê³µì§€ëŠ¥ì€ ëŒ€ëŸ‰ì˜ ë°ì´í„°ë¥¼ ë¹ ë¥´ê²Œ ì²˜ë¦¬í•˜ê³  íŒ¨í„´ì„ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."),
            ("ì§ˆë¬¸: í•œêµ­ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì¸ê°€ìš”?", "í•œêµ­ì˜ ìˆ˜ë„ëŠ” ì„œìš¸ì…ë‹ˆë‹¤."),
        ]
    }
    
    # ê° íƒœìŠ¤í¬ í…ŒìŠ¤íŠ¸
    for task, test_cases in test_data.items():
        test_task(model, tokenizer, task, test_cases, device)
    
    # ì „ì²´ ìš”ì•½
    print("\n" + "="*60)
    print("í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("="*60)
    
    print("\nğŸ’¡ ê´€ì°° ì‚¬í•­:")
    print("1. í•™ìŠµ ì „ê³¼ ë¹„êµí•´ì„œ ì¶œë ¥ í’ˆì§ˆ í™•ì¸")
    print("2. ê° íƒœìŠ¤í¬ë³„ ì„±ëŠ¥ ì°¨ì´ ë¶„ì„")
    print("3. ë” ë§ì€ ë°ì´í„°ì™€ ì—í¬í¬ë¡œ ì„±ëŠ¥ í–¥ìƒ ê°€ëŠ¥")
    print("4. íƒœìŠ¤í¬ë³„ fine-tuningìœ¼ë¡œ ì¶”ê°€ ê°œì„  ê°€ëŠ¥")
    
    print("\nğŸ“ˆ ì„±ëŠ¥ í–¥ìƒ ë°©ë²•:")
    print("1. ë” ë§ì€ í•™ìŠµ ë°ì´í„° ì‚¬ìš© (í˜„ì¬: íƒœìŠ¤í¬ë‹¹ 2ê°œ)")
    print("2. ë” ë§ì€ ì—í¬í¬ í•™ìŠµ (í˜„ì¬: 3 ì—í¬í¬)")
    print("3. Learning rate ì¡°ì •")
    print("4. Beam search íŒŒë¼ë¯¸í„° íŠœë‹")
    print("5. íƒœìŠ¤í¬ë³„ ê°€ì¤‘ì¹˜ ì¡°ì •")
    
    print("\n" + "="*60)


if __name__ == "__main__":
    main()


