# Multi-Task KoBART ì•„í‚¤í…ì²˜ ìƒì„¸ ì„¤ëª…

## ğŸ¯ í”„ë¡œì íŠ¸ ê°œìš”

ì´ í”„ë¡œì íŠ¸ëŠ” **í•˜ë‚˜ì˜ ê³µìœ  ì¸ì½”ë”**ì™€ **4ê°œì˜ íƒœìŠ¤í¬ë³„ ë””ì½”ë”**ë¥¼ ê°€ì§„ ë©€í‹°íƒœìŠ¤í¬ í•™ìŠµ ì•„í‚¤í…ì²˜ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.

## ğŸ“ ì•„í‚¤í…ì²˜ ë‹¤ì´ì–´ê·¸ë¨

### ì „ì²´ êµ¬ì¡°

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Input Text                            â”‚
â”‚                  "ì´ê±° ì¢€ ë„ì™€ì£¼ì„¸ìš”"                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Tokenizer      â”‚
        â”‚   (PreTrained)   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼ input_ids, attention_mask
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                             â”‚
        â”‚   SHARED ENCODER            â”‚
        â”‚   (KoBART Encoder)          â”‚
        â”‚                             â”‚
        â”‚   â€¢ 12 Transformer Layers   â”‚
        â”‚   â€¢ 768 Hidden Dim          â”‚
        â”‚   â€¢ 12 Attention Heads      â”‚
        â”‚   â€¢ ~66M Parameters         â”‚
        â”‚                             â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼ encoder_hidden_states (Context Vector)
                  â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚             â”‚             â”‚             â”‚
    â–¼             â–¼             â–¼             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Decoder â”‚   â”‚Decoder â”‚   â”‚Decoder â”‚   â”‚Decoder â”‚
â”‚Head 1  â”‚   â”‚Head 2  â”‚   â”‚Head 3  â”‚   â”‚Head 4  â”‚
â”‚        â”‚   â”‚        â”‚   â”‚        â”‚   â”‚        â”‚
â”‚Style   â”‚   â”‚Dialogueâ”‚   â”‚ Role   â”‚   â”‚  QA    â”‚
â”‚Transferâ”‚   â”‚Summary â”‚   â”‚ Gen    â”‚   â”‚  Gen   â”‚
â”‚        â”‚   â”‚        â”‚   â”‚        â”‚   â”‚        â”‚
â”‚~103M   â”‚   â”‚~103M   â”‚   â”‚~103M   â”‚   â”‚~103M   â”‚
â”‚params  â”‚   â”‚params  â”‚   â”‚params  â”‚   â”‚params  â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜
     â”‚            â”‚            â”‚            â”‚
     â–¼            â–¼            â–¼            â–¼
  â”Œâ”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”
  â”‚LM   â”‚     â”‚LM   â”‚     â”‚LM   â”‚     â”‚LM   â”‚
  â”‚Head â”‚     â”‚Head â”‚     â”‚Head â”‚     â”‚Head â”‚
  â””â”€â”€â”¬â”€â”€â”˜     â””â”€â”€â”¬â”€â”€â”˜     â””â”€â”€â”¬â”€â”€â”˜     â””â”€â”€â”¬â”€â”€â”˜
     â”‚            â”‚            â”‚            â”‚
     â–¼            â–¼            â–¼            â–¼
  Output1      Output2      Output3      Output4
```

## ğŸ” ì»´í¬ë„ŒíŠ¸ ìƒì„¸

### 1. Shared Encoder (ê³µìœ  ì¸ì½”ë”)

**ìœ„ì¹˜**: `model.shared_encoder`

**êµ¬ì¡°**:
```python
BartEncoder(
  (embed_tokens): Embedding(30000, 768)
  (embed_positions): LearnedPositionalEmbedding(1024, 768)
  (layers): ModuleList(
    (0-11): 12 x BartEncoderLayer(
      (self_attn): BartAttention(...)
      (self_attn_layer_norm): LayerNorm(...)
      (fc1): Linear(768, 3072)
      (fc2): Linear(3072, 768)
      (final_layer_norm): LayerNorm(...)
    )
  )
  (layernorm_embedding): LayerNorm(...)
)
```

**íŒŒë¼ë¯¸í„°**:
- Embedding: 30,000 (vocab) Ã— 768 = 23,040,000
- Positional: 1,024 Ã— 768 = 786,432
- Transformer Layers: ~42,000,000
- **ì´ ì•½ 66M íŒŒë¼ë¯¸í„°**

**ì—­í• **:
1. ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ í† í°ìœ¼ë¡œ ë³€í™˜
2. í† í°ì„ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜
3. 12ê°œ ë ˆì´ì–´ë¥¼ í†µê³¼í•˜ë©° ì»¨í…ìŠ¤íŠ¸ ì¸ì½”ë”©
4. ìµœì¢… hidden states ì¶œë ¥ (context vector)

### 2. Decoder Groups (íƒœìŠ¤í¬ë³„ ë””ì½”ë”)

**ìœ„ì¹˜ / ë§¤í•‘**: 
- `model.decoders['shared_text']` â†’ `style_transfer`, `dialogue_summarization`, `role_generation`
- `model.decoders['qa_generation']` â†’ `qa_generation`

**êµ¬ì¡°** (ê° ë””ì½”ë”):
```python
BartDecoder(
  (embed_tokens): Embedding(30000, 768)
  (embed_positions): LearnedPositionalEmbedding(1024, 768)
  (layers): ModuleList(
    (0-11): 12 x BartDecoderLayer(
      (self_attn): BartAttention(...)
      (encoder_attn): BartAttention(...)  # Cross-attention
      (self_attn_layer_norm): LayerNorm(...)
      (encoder_attn_layer_norm): LayerNorm(...)
      (fc1): Linear(768, 3072)
      (fc2): Linear(3072, 768)
      (final_layer_norm): LayerNorm(...)
    )
  )
  (layernorm_embedding): LayerNorm(...)
)
```

**íŒŒë¼ë¯¸í„°** (ë””ì½”ë” 1ê°œ):
- ì•½ 103M íŒŒë¼ë¯¸í„°
- **í˜„ì¬ êµ¬ì„±:** ê³µìœ  Text ë””ì½”ë” + QA ë””ì½”ë” = ì´ ì•½ 206M

**ì—­í• **:
1. ì¸ì½”ë”ì˜ context vectorë¥¼ ë°›ìŒ
2. Cross-attentionìœ¼ë¡œ ì¸ì½”ë” ì •ë³´ í™œìš©
3. Self-attentionìœ¼ë¡œ ì´ì „ í† í° ì°¸ì¡°
4. íƒœìŠ¤í¬ ê·¸ë£¹ì— íŠ¹í™”ëœ ì¶œë ¥ ìƒì„± (style/summary/roleì€ í•˜ë‚˜ì˜ ë””ì½”ë” ê³µìœ )

### 3. Language Model Heads (LM í—¤ë“œ)

**ìœ„ì¹˜ / ë§¤í•‘**:
- `model.lm_heads['shared_text']` â†’ `style_transfer`, `dialogue_summarization`, `role_generation`
- `model.lm_heads['qa_generation']` â†’ `qa_generation`

**êµ¬ì¡°**:
```python
Linear(768, 30000)  # hidden_size â†’ vocab_size
```

**íŒŒë¼ë¯¸í„°** (í—¤ë“œ 1ê°œ):
- 768 Ã— 30,000 = 23,040,000
- **í˜„ì¬ êµ¬ì„±:** 2ê°œ í—¤ë“œ â†’ ì´ ì•½ 46M íŒŒë¼ë¯¸í„°

**ì—­í• **:
1. ë””ì½”ë” ì¶œë ¥ (768ì°¨ì›)ì„ ì–´íœ˜ í¬ê¸°(30,000)ë¡œ ë³€í™˜
2. ê° í† í°ì˜ í™•ë¥  ë¶„í¬ ìƒì„±
3. ìµœì¢… í† í° ì˜ˆì¸¡

## ğŸ“Š ì „ì²´ íŒŒë¼ë¯¸í„° í†µê³„

| ì»´í¬ë„ŒíŠ¸ | íŒŒë¼ë¯¸í„° ìˆ˜ | ë¹„ìœ¨(ëŒ€ëµ) |
|---------|-----------|-----------|
| Shared Encoder | 66M | 24% |
| Shared Text Decoder | 103M | 37% |
| QA Decoder | 103M | 37% |
| LM Heads (2) | 23M Ã— 2 | 12% |
| **ì´í•©** | **~295M** | **100%** |

## ğŸ”„ ë°ì´í„° íë¦„

### Forward Pass

```python
# 1. ì…ë ¥ ì¤€ë¹„
text = "ì´ê±° ì¢€ ë„ì™€ì£¼ì„¸ìš”"
tokens = tokenizer(text, return_tensors="pt")
# tokens.shape: [batch_size, seq_len]

# 2. ì¸ì½”ë” í†µê³¼
encoder_output = model.shared_encoder(tokens['input_ids'])
# encoder_output.shape: [batch_size, seq_len, 768]

# 3. íƒœìŠ¤í¬ ì„ íƒ
task = 'style_transfer'
decoder_key = model.task_to_decoder[task]
decoder = model.decoders[decoder_key]

# 4. ë””ì½”ë” í†µê³¼
decoder_output = decoder(
    input_ids=decoder_input_ids,
    encoder_hidden_states=encoder_output
)
# decoder_output.shape: [batch_size, target_len, 768]

# 5. LM Head í†µê³¼
logits = model.lm_heads[decoder_key](decoder_output)
# logits.shape: [batch_size, target_len, 30000]

# 6. í† í° ì˜ˆì¸¡
predicted_tokens = torch.argmax(logits, dim=-1)
# predicted_tokens.shape: [batch_size, target_len]

# 7. ë””ì½”ë”©
output_text = tokenizer.decode(predicted_tokens[0])
```

### Training Flow

```python
# 1. ë°ì´í„° ë¡œë“œ
batch = {
    'task': 'style_transfer',
    'input': "ì´ê±° ì¢€ ë„ì™€ì£¼ì„¸ìš”",
    'target': "ì´ê²ƒì„ ë„ì™€ì£¼ì‹œê² ìŠµë‹ˆê¹Œ?"
}

# 2. Forward pass
outputs = model(
    input_ids=input_tokens,
    decoder_input_ids=target_tokens,
    task=batch['task']
)

# 3. Loss ê³„ì‚°
loss = CrossEntropyLoss(outputs['logits'], labels)

# 4. Backward pass
loss.backward()

# 5. íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸
optimizer.step()
```

## ğŸ“ í•™ìŠµ ì „ëµ

### ì „ëµ 1: Joint Training (ë™ì‹œ í•™ìŠµ)

```python
for epoch in range(num_epochs):
    for batch in dataloader:
        task = batch['task']  # ë°°ì¹˜ë§ˆë‹¤ íƒœìŠ¤í¬ê°€ ë‹¤ë¦„
        
        # Forward
        outputs = model(
            input_ids=batch['input_ids'],
            decoder_input_ids=batch['decoder_input_ids'],
            task=task
        )
        
        # Loss & Backward
        loss = compute_loss(outputs, batch['labels'])
        loss.backward()
        optimizer.step()
```

**ì¥ì **:
- ì¸ì½”ë”ê°€ ëª¨ë“  íƒœìŠ¤í¬ë¥¼ ë™ì‹œì— í•™ìŠµ
- íƒœìŠ¤í¬ ê°„ ì§€ì‹ ê³µìœ  íš¨ê³¼
- ë²”ìš©ì ì¸ í‘œí˜„ í•™ìŠµ

**ë‹¨ì **:
- íƒœìŠ¤í¬ ê°„ ê°„ì„­ ê°€ëŠ¥
- ë°ì´í„° ê· í˜• ì¤‘ìš”

### ì „ëµ 2: Sequential Training (ìˆœì°¨ í•™ìŠµ)

```python
tasks = ['style_transfer', 'dialogue_summarization', 
         'role_generation', 'qa_generation']

for task in tasks:
    print(f"Training {task}...")
    
    # í•´ë‹¹ íƒœìŠ¤í¬ ë°ì´í„°ë§Œ ì‚¬ìš©
    task_dataloader = get_task_dataloader(task)
    
    for epoch in range(epochs_per_task):
        for batch in task_dataloader:
            outputs = model(
                input_ids=batch['input_ids'],
                decoder_input_ids=batch['decoder_input_ids'],
                task=task
            )
            
            loss = compute_loss(outputs, batch['labels'])
            loss.backward()
            optimizer.step()
```

**ì¥ì **:
- íƒœìŠ¤í¬ë³„ ì§‘ì¤‘ í•™ìŠµ
- ê°„ì„­ ìµœì†Œí™”
- êµ¬í˜„ ë‹¨ìˆœ

**ë‹¨ì **:
- í•™ìŠµ ì‹œê°„ ê¸¸ì–´ì§
- ì´ì „ íƒœìŠ¤í¬ ë§ê° ê°€ëŠ¥ (Catastrophic Forgetting)

### ì „ëµ 3: Two-Stage Training (2ë‹¨ê³„ í•™ìŠµ)

```python
# Stage 1: ì¸ì½”ë” ê³ ì •, ë””ì½”ë”ë§Œ í•™ìŠµ
model.freeze_encoder()

for task in tasks:
    decoder_optimizer = AdamW(
        model.get_decoder_parameters(task),
        lr=5e-5
    )
    
    # íƒœìŠ¤í¬ë³„ ë””ì½”ë” í•™ìŠµ
    train_decoder(model, task, decoder_optimizer)

# Stage 2: ì „ì²´ fine-tuning
model.unfreeze_encoder()
full_optimizer = AdamW(model.parameters(), lr=1e-5)

# ì „ì²´ ëª¨ë¸ ë¯¸ì„¸ ì¡°ì •
fine_tune_all(model, full_optimizer)
```

**ì¥ì **:
- ë¹ ë¥¸ ì´ˆê¸° í•™ìŠµ
- ì•ˆì •ì ì¸ ìˆ˜ë ´
- ë¦¬ì†ŒìŠ¤ íš¨ìœ¨ì 

**ë‹¨ì **:
- 2ë‹¨ê³„ ê´€ë¦¬ í•„ìš”
- Stage 1ì—ì„œ ì¸ì½”ë” ê°œì„  ë¶ˆê°€

## ğŸ”§ ê³ ê¸‰ ê¸°ë²•

### 1. Gradient Accumulation

```python
accumulation_steps = 4

for i, batch in enumerate(dataloader):
    outputs = model(...)
    loss = compute_loss(outputs, labels)
    loss = loss / accumulation_steps
    loss.backward()
    
    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()
```

### 2. Mixed Precision Training

```python
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()

for batch in dataloader:
    with autocast():
        outputs = model(...)
        loss = compute_loss(outputs, labels)
    
    scaler.scale(loss).backward()
    scaler.step(optimizer)
    scaler.update()
```

### 3. Task Sampling Strategy

```python
# íƒœìŠ¤í¬ë³„ ìƒ˜í”Œë§ í™•ë¥  ì¡°ì •
task_weights = {
    'style_transfer': 0.25,
    'dialogue_summarization': 0.35,  # ë” ë§ì´ ìƒ˜í”Œë§
    'role_generation': 0.20,
    'qa_generation': 0.20
}

sampler = WeightedTaskSampler(dataset, task_weights)
dataloader = DataLoader(dataset, sampler=sampler)
```

## ğŸ’¾ ëª¨ë¸ ì €ì¥ ë° ë¡œë“œ

### ì „ì²´ ëª¨ë¸ ì €ì¥

```python
torch.save({
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
    'epoch': epoch,
    'loss': loss,
}, 'multi_task_kobart.pt')
```

### íƒœìŠ¤í¬ë³„ ë””ì½”ë”ë§Œ ì €ì¥

```python
task = 'style_transfer'
torch.save({
    'decoder_state_dict': model.decoders[task].state_dict(),
    'lm_head_state_dict': model.lm_heads[task].state_dict(),
}, f'{task}_decoder.pt')
```

### ë¡œë“œ

```python
checkpoint = torch.load('multi_task_kobart.pt')
model.load_state_dict(checkpoint['model_state_dict'])
optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
```

## ğŸ¯ ì‹¤ì „ í™œìš© ì˜ˆì œ

### ì±—ë´‡ ì‹œìŠ¤í…œ

```python
def chatbot_pipeline(user_input, user_role='friend'):
    # 1. ì˜ë„ ë¶„ë¥˜
    intent = classify_intent(user_input)
    
    # 2. íƒœìŠ¤í¬ ë§¤í•‘
    task_map = {
        'question': 'qa_generation',
        'chat': 'role_generation',
        'summarize': 'dialogue_summarization',
        'formalize': 'style_transfer'
    }
    task = task_map[intent]
    
    # 3. ì—­í•  ì¶”ê°€
    if task == 'role_generation':
        user_input = f"[{user_role}] {user_input}"
    
    # 4. ìƒì„±
    inputs = tokenizer(user_input, return_tensors="pt")
    outputs = model.generate(
        input_ids=inputs['input_ids'],
        task=task
    )
    
    return tokenizer.decode(outputs[0], skip_special_tokens=True)
```

## ğŸ“ˆ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

```python
def evaluate_all_tasks(model, test_dataloaders):
    results = {}
    
    for task, dataloader in test_dataloaders.items():
        metrics = evaluate_task(model, dataloader, task)
        results[task] = metrics
        
        print(f"{task}:")
        print(f"  Loss: {metrics['loss']:.4f}")
        print(f"  BLEU: {metrics['bleu']:.4f}")
        print(f"  ROUGE: {metrics['rouge']:.4f}")
    
    return results
```

---

**ì‘ì„±ì¼**: 2025-11-16
**ë²„ì „**: 1.0
**ì €ì**: Multi-Task KoBART Team

