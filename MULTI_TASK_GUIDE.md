# Multi-Task KoBART ì•„í‚¤í…ì²˜ ê°€ì´ë“œ

## ğŸ—ï¸ ì•„í‚¤í…ì²˜ êµ¬ì¡°

```
                    ì…ë ¥ ë¬¸ì¥
                        â†“
                   [Tokenizer]
                        â†“
                   Input IDs
                        â†“
        â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
        â•‘   Shared Encoder (KoBART)    â•‘  â† ëª¨ë“  íƒœìŠ¤í¬ê°€ ê³µìœ 
        â•‘   - 12 layers                 â•‘
        â•‘   - 768 hidden size           â•‘
        â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                        â†“
                Context Vector
                        â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚               â”‚               â”‚
        â†“               â†“               â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚Decoder â”‚      â”‚Decoder â”‚     â”‚Decoder â”‚     â”‚Decoder â”‚
    â”‚Head 1  â”‚      â”‚Head 2  â”‚     â”‚Head 3  â”‚     â”‚Head 4  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“               â†“               â†“             â†“
  Style          Dialogue        Role-based       QA Answer
  Transfer       Summary         Response         Generation
```

## ğŸ“Š êµ¬ì„± ìš”ì†Œ

### 1. Shared Encoder (ê³µìœ  ì¸ì½”ë”)
- **ì—­í• **: ì…ë ¥ ë¬¸ì¥ì„ ì»¨í…ìŠ¤íŠ¸ ë²¡í„°ë¡œ ì¸ì½”ë”©
- **íŠ¹ì§•**: 
  - ëª¨ë“  4ê°œ íƒœìŠ¤í¬ì—ì„œ ê³µìœ 
  - ë©€í‹°íƒœìŠ¤í¬ í•™ìŠµì„ í†µí•´ ë²”ìš©ì  í‘œí˜„ í•™ìŠµ
  - ì•½ 6ì²œë§Œ ê°œ íŒŒë¼ë¯¸í„°
- **ì¥ì **:
  - ë©”ëª¨ë¦¬ íš¨ìœ¨ì 
  - íƒœìŠ¤í¬ ê°„ ì§€ì‹ ê³µìœ 
  - ì „ì´ í•™ìŠµ íš¨ê³¼

### 2. Task-Specific Decoders (íƒœìŠ¤í¬ë³„ ë””ì½”ë”)

#### Head 1: Style Transfer (ìŠ¤íƒ€ì¼ ë³€í™˜)
- **ëª©ì **: ë¬¸ì¥ì˜ ìŠ¤íƒ€ì¼ ë³€í™˜ (êµ¬ì–´ì²´ â†” ê²©ì‹ì²´)
- **ì˜ˆì‹œ**:
  - ì…ë ¥: "ì´ê±° ì¢€ ë„ì™€ì£¼ì„¸ìš”"
  - ì¶œë ¥: "ì´ê²ƒì„ ë„ì™€ì£¼ì‹œê² ìŠµë‹ˆê¹Œ?"

#### Head 2: Dialogue Summarization (ëŒ€í™” ìš”ì•½)
- **ëª©ì **: ëŒ€í™” ë‚´ìš©ì„ ê°„ê²°í•˜ê²Œ ìš”ì•½
- **ì˜ˆì‹œ**:
  - ì…ë ¥: "A: ë‚´ì¼ íšŒì˜ ëª‡ ì‹œì—ìš”? B: ì˜¤í›„ 2ì‹œì…ë‹ˆë‹¤."
  - ì¶œë ¥: "ë‚´ì¼ íšŒì˜ëŠ” ì˜¤í›„ 2ì‹œì…ë‹ˆë‹¤."

#### Head 3: Role-conditioned Generation (ì—­í•  ê¸°ë°˜ ì‘ë‹µ)
- **ëª©ì **: íŠ¹ì • ì—­í• ì— ë§ëŠ” ì‘ë‹µ ìƒì„±
- **ì˜ˆì‹œ**:
  - ì…ë ¥: "[ì„ ìƒë‹˜] íŒŒì´ì¬ì´ë€?"
  - ì¶œë ¥: "íŒŒì´ì¬ì€ ë°°ìš°ê¸° ì‰¬ìš´ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤."

#### Head 4: QA Answer Generation (QA ë‹µë³€ ìƒì„±)
- **ëª©ì **: ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„±
- **ì˜ˆì‹œ**:
  - ì…ë ¥: "ì§ˆë¬¸: ì„œìš¸ì˜ ì¸êµ¬ëŠ”?"
  - ì¶œë ¥: "ì„œìš¸ì˜ ì¸êµ¬ëŠ” ì•½ 1ì²œë§Œ ëª…ì…ë‹ˆë‹¤."

## ğŸš€ ì‚¬ìš© ë°©ë²•

### ê¸°ë³¸ ì‚¬ìš©ë²•

```python
from multi_task_kobart import MultiTaskKoBART
from transformers import PreTrainedTokenizerFast
import torch

# ëª¨ë¸ ë¡œë“œ
tokenizer = PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v1')
model = MultiTaskKoBART()

# ì…ë ¥ ì¤€ë¹„
text = "ì´ê±° ì¢€ ë„ì™€ì£¼ì„¸ìš”."
inputs = tokenizer(text, return_tensors="pt")

# íƒœìŠ¤í¬ë³„ ìƒì„±
output = model.generate(
    input_ids=inputs['input_ids'],
    task='style_transfer',  # íƒœìŠ¤í¬ ì„ íƒ
    max_length=50
)

result = tokenizer.decode(output[0], skip_special_tokens=True)
print(result)
```

### íƒœìŠ¤í¬ë³„ ì‚¬ìš© ì˜ˆì œ

#### 1. Style Transfer
```python
text = "ë¹¨ë¦¬ ì™€"
output = model.generate(
    input_ids=tokenizer(text, return_tensors="pt")['input_ids'],
    task='style_transfer',
    max_length=50
)
# Expected: "ë¹ ë¥¸ ì‹œì¼ ë‚´ì— ë°©ë¬¸í•´ ì£¼ì‹œê¸° ë°”ëë‹ˆë‹¤."
```

#### 2. Dialogue Summarization
```python
dialogue = "A: ì ì‹¬ ë­ ë¨¹ì„ê¹Œìš”? B: í•œì‹ì´ ì¢‹ê² ì–´ìš”. A: ì¢‹ì•„ìš”."
output = model.generate(
    input_ids=tokenizer(dialogue, return_tensors="pt")['input_ids'],
    task='dialogue_summarization',
    max_length=50
)
# Expected: "ì ì‹¬ìœ¼ë¡œ í•œì‹ì„ ë¨¹ê¸°ë¡œ í–ˆìŠµë‹ˆë‹¤."
```

#### 3. Role-conditioned Generation
```python
prompt = "[ì¹œêµ¬] ì£¼ë§ì— ë­í•´?"
output = model.generate(
    input_ids=tokenizer(prompt, return_tensors="pt")['input_ids'],
    task='role_generation',
    max_length=50
)
# Expected: "íŠ¹ë³„í•œ ê³„íšì€ ì—†ì–´. ë„ˆëŠ”?"
```

#### 4. QA Answer Generation
```python
question = "ì§ˆë¬¸: ì¸ê³µì§€ëŠ¥ì˜ ì¥ì ì€?"
output = model.generate(
    input_ids=tokenizer(question, return_tensors="pt")['input_ids'],
    task='qa_generation',
    max_length=50
)
# Expected: "ì¸ê³µì§€ëŠ¥ì€ ëŒ€ëŸ‰ì˜ ë°ì´í„°ë¥¼ ë¹ ë¥´ê²Œ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
```

## ğŸ“ í•™ìŠµ ë°©ë²•

### 1. ì „ì²´ í•™ìŠµ (End-to-End)
```python
from train_multi_task import main

# ì „ì²´ íŒŒë¼ë¯¸í„° í•™ìŠµ
main()
```

### 2. ì¸ì½”ë” ê³ ì • í•™ìŠµ
```python
# ì¸ì½”ë” ê³ ì •, ë””ì½”ë”ë§Œ í•™ìŠµ
model = MultiTaskKoBART()
model.freeze_encoder()

# ì´í›„ í•™ìŠµ ì§„í–‰
```

### 3. íƒœìŠ¤í¬ë³„ í•™ìŠµ
```python
# íŠ¹ì • íƒœìŠ¤í¬ë§Œ í•™ìŠµ
task = 'style_transfer'
optimizer = torch.optim.AdamW(
    model.get_decoder_parameters(task),
    lr=5e-5
)
```

## ğŸ“ˆ í•™ìŠµ ì „ëµ

### 1. ìˆœì°¨ í•™ìŠµ (Sequential Training)
```
Epoch 1-10: Task 1 í•™ìŠµ
Epoch 11-20: Task 2 í•™ìŠµ
Epoch 21-30: Task 3 í•™ìŠµ
Epoch 31-40: Task 4 í•™ìŠµ
```

### 2. ë™ì‹œ í•™ìŠµ (Simultaneous Training)
```
ê° ë°°ì¹˜ë§ˆë‹¤ 4ê°œ íƒœìŠ¤í¬ë¥¼ ë²ˆê°ˆì•„ê°€ë©° í•™ìŠµ
- íƒœìŠ¤í¬ ê· í˜• ìœ ì§€ ì¤‘ìš”
- ë°ì´í„° ìƒ˜í”Œë§ ì „ëµ í•„ìš”
```

### 3. 2ë‹¨ê³„ í•™ìŠµ (Two-Stage Training)
```
Stage 1: ì¸ì½”ë” ê³ ì •, ë””ì½”ë”ë§Œ í•™ìŠµ (ë¹ ë¦„)
Stage 2: ì „ì²´ fine-tuning (ì •í™•ë„ í–¥ìƒ)
```

## ğŸ’¾ ë°ì´í„° ì¤€ë¹„

### ë°ì´í„° í¬ë§·
```python
train_data = [
    {
        'task': 'style_transfer',
        'input': 'ì…ë ¥ í…ìŠ¤íŠ¸',
        'target': 'íƒ€ê²Ÿ í…ìŠ¤íŠ¸'
    },
    # ... ë” ë§ì€ ë°ì´í„°
]
```

### ê¶Œì¥ ë°ì´í„° í¬ê¸°
- **ìµœì†Œ**: íƒœìŠ¤í¬ë‹¹ 1,000ê°œ
- **ê¶Œì¥**: íƒœìŠ¤í¬ë‹¹ 10,000ê°œ ì´ìƒ
- **ìµœì **: íƒœìŠ¤í¬ë‹¹ 100,000ê°œ ì´ìƒ

### ë°ì´í„° ê· í˜•
- 4ê°œ íƒœìŠ¤í¬ì˜ ë°ì´í„° ë¹„ìœ¨ì„ ë¹„ìŠ·í•˜ê²Œ ìœ ì§€
- ë¶ˆê· í˜• ì‹œ ìƒ˜í”Œë§ ê°€ì¤‘ì¹˜ ì¡°ì •

## âš™ï¸ ê³ ê¸‰ ì„¤ì •

### 1. íƒœìŠ¤í¬ë³„ ê°€ì¤‘ì¹˜
```python
task_weights = {
    'style_transfer': 1.0,
    'dialogue_summarization': 1.5,
    'role_generation': 1.2,
    'qa_generation': 1.0
}

# Lossì— ê°€ì¤‘ì¹˜ ì ìš©
weighted_loss = loss * task_weights[task]
```

### 2. í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§
```python
from torch.optim.lr_scheduler import CosineAnnealingLR

scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs)
```

### 3. Gradient Accumulation
```python
accumulation_steps = 4

for step, batch in enumerate(dataloader):
    loss = train_step(model, batch, optimizer, device)
    loss = loss / accumulation_steps
    loss.backward()
    
    if (step + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()
```

## ğŸ“Š ì„±ëŠ¥ í‰ê°€

### íƒœìŠ¤í¬ë³„ ë©”íŠ¸ë¦­

| Task | Metric | ì„¤ëª… |
|------|--------|------|
| Style Transfer | BLEU, Style Accuracy | ìŠ¤íƒ€ì¼ ë³€í™˜ ì •í™•ë„ |
| Dialogue Summary | ROUGE, BERTScore | ìš”ì•½ í’ˆì§ˆ |
| Role Generation | Perplexity, Human Eval | ìì—°ìŠ¤ëŸ¬ì›€ |
| QA Generation | F1, EM | ë‹µë³€ ì •í™•ë„ |

## ğŸ”§ ë¬¸ì œ í•´ê²°

### 1. ë©”ëª¨ë¦¬ ë¶€ì¡±
- ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸°
- Gradient checkpointing ì‚¬ìš©
- í•œ ë²ˆì— í•œ íƒœìŠ¤í¬ì”© í•™ìŠµ

### 2. íŠ¹ì • íƒœìŠ¤í¬ ì„±ëŠ¥ ì €í•˜
- í•´ë‹¹ íƒœìŠ¤í¬ ë°ì´í„° ì¦ê°•
- íƒœìŠ¤í¬ë³„ í•™ìŠµë¥  ì¡°ì •
- í•´ë‹¹ ë””ì½”ë”ë§Œ ì¶”ê°€ í•™ìŠµ

### 3. íƒœìŠ¤í¬ ê°„ ê°„ì„­
- ì¸ì½”ë” ê³ ì • í›„ ë””ì½”ë”ë§Œ í•™ìŠµ
- íƒœìŠ¤í¬ë³„ ìˆœì°¨ í•™ìŠµ
- Adapter ë ˆì´ì–´ ì¶”ê°€

## ğŸ“š ì°¸ê³  ìë£Œ

- **ë…¼ë¬¸**: "Multi-Task Learning with Deep Neural Networks"
- **KoBART**: https://huggingface.co/gogamza/kobart-base-v1
- **Transformers**: https://huggingface.co/docs/transformers

## ğŸ¯ ì‹¤ì „ í™œìš©

### 1. ì±—ë´‡ ì‹œìŠ¤í…œ
```
ì…ë ¥ â†’ ì˜ë„ ë¶„ë¥˜ â†’ ì ì ˆí•œ ë””ì½”ë” ì„ íƒ â†’ ì‘ë‹µ ìƒì„±
```

### 2. ë¬¸ì„œ ì²˜ë¦¬ ì‹œìŠ¤í…œ
```
ë¬¸ì„œ â†’ ìš”ì•½(Head 2) â†’ ìŠ¤íƒ€ì¼ ë³€í™˜(Head 1) â†’ ìµœì¢… ë¬¸ì„œ
```

### 3. QA ì‹œìŠ¤í…œ
```
ì§ˆë¬¸ â†’ QA ìƒì„±(Head 4) â†’ ì—­í•  ê¸°ë°˜ ë‹µë³€(Head 3)
```

---

**Created**: 2025-11-16
**Version**: 1.0
**License**: MIT

