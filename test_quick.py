"""
ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ëª¨ë¸ í‰ê°€
"""

import torch
from transformers import PreTrainedTokenizerFast
from multi_task_kobart import MultiTaskKoBART


def load_model(checkpoint_path: str, device):
    """í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ"""
    print("ëª¨ë¸ ë¡œë”© ì¤‘...")
    
    model = MultiTaskKoBART()
    
    # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
    checkpoint = torch.load(checkpoint_path, map_location=device)
    model.load_state_dict(checkpoint['model_state_dict'])
    model.to(device)
    model.eval()
    
    print("âœ“ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    if 'epoch' in checkpoint:
        print(f"  - Epoch: {checkpoint['epoch']}")
    if 'train_loss' in checkpoint:
        print(f"  - Train Loss: {checkpoint['train_loss']:.4f}")
    if 'dev_loss' in checkpoint:
        print(f"  - Dev Loss: {checkpoint['dev_loss']:.4f}")
    
    return model


def generate_text(model, tokenizer, input_text, source_style, target_style, device, max_length=128):
    """í…ìŠ¤íŠ¸ ìƒì„±"""
    # ì…ë ¥ í˜•ì‹: [sourceâ†’target] text
    formatted_input = f"[{source_style}â†’{target_style}] {input_text}"
    
    # í† í°í™”
    inputs = tokenizer(
        formatted_input,
        return_tensors="pt",
        max_length=max_length,
        truncation=True
    )
    inputs = {k: v.to(device) for k, v in inputs.items()}
    
    # ìƒì„±
    with torch.no_grad():
        outputs = model.generate(
            input_ids=inputs['input_ids'],
            attention_mask=inputs['attention_mask'],
            task='style_transfer',
            max_length=max_length,
            repetition_penalty=3.0,
            no_repeat_ngram_size=2,
            num_beams=5,
            early_stopping=True
        )
    
    # ë””ì½”ë”©
    result = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return result


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("="*60)
    print("ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ëª¨ë¸ í‰ê°€")
    print("="*60)
    print()
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ë””ë°”ì´ìŠ¤: {device}\n")
    
    # í† í¬ë‚˜ì´ì € ë¡œë“œ
    print("í† í¬ë‚˜ì´ì € ë¡œë”© ì¤‘...")
    tokenizer = PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v1')
    print("âœ“ í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ\n")
    
    # ëª¨ë¸ ë¡œë“œ
    checkpoint_path = "checkpoints/quick_test_epoch_3.pt"
    
    try:
        model = load_model(checkpoint_path, device)
    except FileNotFoundError:
        print(f"âŒ ì²´í¬í¬ì¸íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {checkpoint_path}")
        return
    
    print("\n" + "="*60)
    print("í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ìƒì„±")
    print("="*60)
    
    style_names = {
        'ban': 'ë°˜ë§',
        'yo': 'ìš”ì²´',
        'sho': 'í•©ì‡¼ì²´'
    }
    
    test_cases = [
        # ë°˜ë§ â†’ ìš”ì²´
        {
            'input': 'ì•ˆë…•. ì˜¤ëŠ˜ ë‚ ì”¨ ì¢‹ì•„.',
            'source': 'ban',
            'target': 'yo',
        },
        {
            'input': 'ë­í•´? ì‹¬ì‹¬í•´.',
            'source': 'ban',
            'target': 'yo',
        },
        
        # ë°˜ë§ â†’ í•©ì‡¼ì²´
        {
            'input': 'ì´ê±° ì¢€ ë„ì™€ì¤˜.',
            'source': 'ban',
            'target': 'sho',
        },
        {
            'input': 'íšŒì˜ ì‹œì‘í–ˆì–´.',
            'source': 'ban',
            'target': 'sho',
        },
        
        # ìš”ì²´ â†’ ë°˜ë§
        {
            'input': 'ë„¤. ì•Œê² ì–´ìš”.',
            'source': 'yo',
            'target': 'ban',
        },
        {
            'input': 'ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì¢‹ì•„ìš”.',
            'source': 'yo',
            'target': 'ban',
        },
        
        # ìš”ì²´ â†’ í•©ì‡¼ì²´
        {
            'input': 'íšŒì˜ê°€ ìˆì–´ìš”.',
            'source': 'yo',
            'target': 'sho',
        },
        
        # í•©ì‡¼ì²´ â†’ ë°˜ë§
        {
            'input': 'ì¤€ë¹„í•´ì£¼ì‹­ì‹œì˜¤.',
            'source': 'sho',
            'target': 'ban',
        },
        
        # í•©ì‡¼ì²´ â†’ ìš”ì²´
        {
            'input': 'í™•ì¸í•´ì£¼ì‹œê¸° ë°”ëë‹ˆë‹¤.',
            'source': 'sho',
            'target': 'yo',
        },
    ]
    
    for i, test in enumerate(test_cases, 1):
        print(f"\n[í…ŒìŠ¤íŠ¸ {i}] {style_names[test['source']]} â†’ {style_names[test['target']]}")
        print(f"ì…ë ¥: {test['input']}")
        
        result = generate_text(
            model, tokenizer,
            test['input'],
            test['source'],
            test['target'],
            device
        )
        print(f"ì¶œë ¥: {result}")
    
    print("\n" + "="*60)
    print("í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("="*60)
    
    print("\nğŸ“Š ê´€ì°°:")
    print("  - 2 ì—í¬í¬ë§Œ í•™ìŠµí–ˆì§€ë§Œ íŒ¨í„´ì„ í•™ìŠµí•˜ê¸° ì‹œì‘")
    print("  - ë” ë§ì€ ì—í¬í¬ë¡œ ì„±ëŠ¥ í–¥ìƒ ê°€ëŠ¥")
    print("  - ì „ì²´ ë°ì´í„°(70K)ë¡œ í•™ìŠµí•˜ë©´ ë” ì¢‹ì€ ê²°ê³¼")
    
    print("\nğŸ’¡ ë‹¤ìŒ ë‹¨ê³„:")
    print("  1. ì „ì²´ ë°ì´í„°ë¡œ í•™ìŠµ: train_style_transfer.py")
    print("  2. ë” ë§ì€ ì—í¬í¬ (10-20)")
    print("  3. Beam search íŒŒë¼ë¯¸í„° íŠœë‹")


if __name__ == "__main__":
    main()


